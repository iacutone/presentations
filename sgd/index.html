<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

![jpeg](images/qr_code.png)

---

class: center, middle

# Learn Stochastic Gradient Descent (SGD) In Under 30 Minutes

---

class: center, middle

# Welcome

![gif](images/hello.gif)

---

class: center, middle

# Welcome

![jpeg](images/avatar.jpeg)

---
class: center, middle

![svg](images/qr-cars.png)

---

# Why Learn SGD?

- SGD an integral building block of deep neural networks 

---

# Why Learn SGD?

"Micrograd is what you need to train neural networks and everything else is just efficiency." - Andrej Karpathy

---

# Why Learn SGD?


```elixir
Axon.Loop.trainer(:mean_squared_error, :sgd)
```

---

class: center, middle

![svg](images/struggle.png)

---

![svg](images/nx.png)
![svg](images/axon.png)

---

# Learning From First Principles

![svg](images/before.jpeg)

---

# Learning From First Principles

![svg](images/bird.png)

---

# Learning From First Principles

![svg](images/ref-highlight.png)


---

# Learning From First Principles

![svg](images/after.jpeg)

---

# What does SGD mean?

--

## Stochastic:
- random

--

## Gradient:
- slope of a function

--

## Descent:
- moving downwards

---

# What does SGD mean?

- SGD is an optimization function that eventually finds the lowest point (minumum) of a function

---

class: center, middle

## SGD in Action

![svg](images/sgd.gif)

---
class: center, middle

![jpeg](images/qr_code.png)

---

# Calculus Ingredients

## The Derivative

![svg](images/CodeCogsEqn.svg)

--

y = 2x

--

dy/dx = (2(x+h) - 2x) / h 

--

      = 2x + 2h - 2x / h

--

      = 2h / h

--

      = 2

- In other words, the derivative tells us the sensitivity of the function to change

---

# Calculus Ingredients

.lg[![png](images/2x-function.png)]

---

# Calculus Ingredients

## The Chain Rule

![png](images/chainrule.png)

--

"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man." - [George F. Simmons](https://en.wikipedia.org/wiki/Chain_rule)

--

We can take advatage of the chain rule to calculate the gradient of a neural network via the derivative.

---

# LiveBook Deep Dive

## Example Node Struct

```elixir
  @type t :: %Node{
    data: Integer.t() | nil,
    grad: Integer.t(),
    label: String.t(),
    _backward: fun(any),
    _op: String.t(),
    _children: list(%Node{})
  }
```

- it's a tree data structure with children nodes

---

# How is SGD using derivatives?

## Node Module API

### Addition

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.add(a, b)
```

.lg[![png](images/add-graph.png)]

---

# How is SGD using derivatives?

## Node Module API

### Multiplication

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.mult(a, b)
```

.lg[![png](images/multi-graph.png)]

---

# How is SGD using derivatives?

### Multiplication then Addition

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
c = %Node1{data: 10, label: "c"}
d = Node1.mult(a, b, "e") |> Node1.add(c, "d", 1)
```

.lg[![png](images/nodes.png)]

---

# How is SGD using derivatives?

.lg[![png](images/grads.png)]

---

# How is SGD using derivatives?

- 3 * -3 + 10 = 1

- the data value of the parent node decreased from 4 to 1 which corresponds to the gradient of Node a

.lg[![png](images/grads.png)]

---

# Nx 

## Numerical Elixir

---

# Nx

## Tensor

### Rank 0 Tensor

- a single scalar number

```elixir
Nx.tensor([1])
```

---

# Nx

## Tensor

### Rank 1 Tensor

- a list of numbers

```elixir
Nx.tensor([1, 2, 3])
```

---

# Nx

## Tensor

### Rank 2 Tensor

- a table of numbers

```elixir
Nx.tensor([[1], [2]])
```

---

# Nx

## Tensor

### Rank 3 Tensor

- a table of tables of numbers

---

# Numerical Function (defn)

- Allows us to compute tensor values faster with JIT compilation on the GPU

---

![png](images/2x.png)

---

# SGD Ingredients

### w and b
### Learning Rate
### Loss Function

---

# w and b

- randomly selected numbers that will be updated by SGD


```elixir
defp random_parameters do
  key = Nx.Random.key(12)
  {w, _new_key} = Nx.Random.normal(key, 0.0, 1.0)
  {b, _new_key} = Nx.Random.normal(key, 0.0, 1.0)

  {w, b}
end
```

- w and b are two random floats between 0 and 1

```elixir
{0.4221189078, 0.3289251856}
```

---

# Learning Rate

- a hyperparameter that controls how much to update the w and b activations

```elixir
@learning_rate 0.01

{
  w - grad_w * @learning_rate,
  b - grad_b * @learning_rate
}
```  

---

# Learning Rate

![png](images/lr-low.png)

---

# Learning Rate

![png](images/lr-high.png)

---

# Loss Function

- let's you know how far the prediction is off from the actual value
- mean squared error is a common loss function

--

```elixir
defn mse(activations, x, y) do
  y_hat = predict(activations, x)

  (y - y_hat)
  |> Nx.pow(2)
  |> Nx.mean()
end
```

---

![png](images/mse.png)

---

# Loss Function

![png](images/mse-1.png)

---

# Update

- process of calculating the gradients and updating the w and b activations

```elixir
  defn update({w, b} = activations, x, y) do
    {grad_w, grad_b} = grad(activations, &mse(&1, x, y))

    {
      w - grad_w * @learning_rate,
      b - grad_b * @learning_rate
    }
  end
```

---

# Update

![png](images/forward.png)

---

<video width="700" height="700" controls autoplay>
  <source src="images/manual.mov" type="video/mp4">
</video>

---

# Training - All Together Now

- a convenience function to loop over the data and update the parameters (w and b)

```elixir
  def train(data, epochs \\ 100) do
    Enum.reduce(1..epochs, random_parameters(), fn _i, acc ->
      data
      |> Enum.take(@batch_size)
      |> Enum.reduce(acc, fn batch, activations ->
          {x, y} = Enum.unzip(batch)
          x = Nx.tensor(x)
          y = Nx.tensor(y)
          update(activations, x, y)
        end)
    end)
  end
 ``` 

---

# Training - All Together Now

![png](images/fin.png)

---

# Training - All Together Now

## After 5 Epochs

```elixir
activations: {1.9050873517990112, 0.6578895449638367}
activations: {1.9301851987838745, 0.47839775681495667}
activations: {1.9488297700881958, 0.34842410683631897}
activations: {1.959909439086914, 0.2548447549343109}
activations: {1.9717564582824707, 0.1854221224784851}
```

---

# Axon

```elixir
Axon.Loop.trainer(:mean_squared_error, :sgd)
```

---

# Neural Network

![png](images/nn.png)


---

# Additional Resources

- [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) -- Andrej Karpathy
- [fast.ai](https://www.fast.ai/) -- Jeremy Howard
- [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work) -- Jeremy Howard
- [Nx](https://hex.pm/packages/nx)
- [Axon](https://hex.pm/packages/axon)
- [Evolve Artist](https://evolveartist.com/)
- [Up and Running Nx](https://dockyard.com/blog/2021/04/08/up-and-running-nx) -- Dockyard, Sean Moriarity
- [My LiveBook GH Repo](https://github.com/iacutone/presentations/tree/master/sgd)
- [My website](https://iacut.one/)

---

class: center, middle

# FIN


    </textarea>
    <script src="../remark.min.js"></script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>


<style>
  img {
    max-width: 85%;
    max-height: 85%;
 }

  .lg img {
    position: absolute;
    object-fit: cover;
    width: 100%;
  }
</style>
