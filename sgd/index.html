<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

![jpeg](images/qr_code.png)

---

class: center, middle

# Learn Stochastic Gradient Descent in 30 Minutes

---

# Welcome

![gif](images/hello.gif)

---

# Welcome

![jpeg](images/avatar.jpeg)

---
class: center, middle

# Intro


![svg](images/icon-cars-app.png)

---

# Why Learn SGD?

--

- SGD is the building block of deep neural networks 

--

- SGD is arguably the most important piece of deep neural networks

--

- "Micrograd is what you need to train neural networks and everything else is just efficiency" - Andrej Karpathy

--

```elixir
Axon.Loop.trainer(:mean_squared_error, :sgd)
```

---

# Learning From First Principles

![svg](images/before.jpeg)

---

# Learning From First Principles

![svg](images/bird.png)

---

# Learning From First Principles

![svg](images/after.jpeg)

---

# What does SGD mean?

--

## Stochastic:
- random

--

## Gradient:
- slope of a function

--

## Descent:
- moving downwards

---

# What does SGD mean?

- SGD is an optimization function that finds the lowest point of a function

---

## SGD in Action

![svg](images/sgd.gif)

---
class: center, middle

![jpeg](images/qr_code.png)

---

# Calculus Ingredients

## The Derivative

![svg](images/CodeCogsEqn.svg)

--

y = 2x

--

dy/dx = (2(x+h) - 2x) / h 

--

      = 2x + 2h - 2x / h

--

      = 2h / h

--

      = 2

- In other words, the derivative tells us the sensitivity of the function to change

---

# Calculus Ingredients

.lg[![png](images/2x-function.png)]

---

# SGD Ingredients

## The Derivative

![svg](images/CodeCogsEqn.svg)

### Applied in SGD

- we will use this fundamental rule of calculus to calculate the gradient of a neural network

---

# Calculus Ingredients

## The Chain Rule

![png](images/chainrule.png)

--

"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man." - [George F. Simmons](https://en.wikipedia.org/wiki/Chain_rule)

--

We can take advatage of the chain rule to calculate the gradient of a neural network via the derivative.

---

# LiveBook Deep Dive

## Example Node Struct

```elixir
  @type t :: %Node{
    data: Integer.t() | nil,
    grad: Integer.t(),
    label: String.t(),
    _backward: fun(any),
    _op: String.t(),
    _children: list(%Node{})
  }
```

- it's a tree data structure with childred nodes

---

# How is SGD using derivatives?

## Node Module API

### Addition

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.add(a, b)

# inserts a new Node with data value of -1
```

---

# How is SGD using derivatives?

## Node Module API

### Multiplication

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.mult(a, b)

# inserts a new Node with data value of -6
```

---

# How is SGD using derivatives?

### Multiplication then Addition, Graphed

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
c = %Node1{data: 10, label: "c"}
d = Node1.mult(a, b, "e") |> Node1.add(c, "d", 1)
```

.lg[![png](images/nodes.png)]

---

# How is SGD using derivatives?

.lg[![png](images/grads.png)]

---

# Nx: 

## Numerical Elixir

---

![png](images/2x.png)

---

# SGD Ingredients

### x and b
### Learning Rate
### Loss Function

---

# w and b

- randomly selected numbers that will be updated by SGD


```elixir
defp random_parameters do
  key = Nx.Random.key(12)
  {w, _new_key} = Nx.Random.normal(key, 0.0, 1.0)
  {b, _new_key} = Nx.Random.normal(key, 0.0, 1.0)

  {w, b}
end
```

- w and b are two random floats between 0 and 1

```elixir
{0.4221189078, 0.3289251856}
```

---

# Learning Rate

- a hyperparameter that controls how much to update the w and b parameters

```elixir
{w - grad_w * @learning_rate,
b - grad_b * @learning_rate}
```  

---

# Learning Rate

![png](images/lr-low.png)

---

# Learning Rate

![png](images/lr-high.png)

---

# Loss Function

- let's you know how far the prediction is off from the actual value

--

- mean squared error is a common loss function

--

```elixir
  defn mse(activations, x, y) do
    y_hat = predict(activations, x)

    (y - y_hat)
    |> Nx.pow(2)
    |> Nx.mean()
  end
```

---

![png](images/mse.png)

---

# Loss Function

![png](images/mse-1.png)

---

# Update

- process of calculating the gradients and updating the w and b parameters

```elixir
  defn update({w, b} = params, x, y) do
    {grad_w, grad_b} = grad(params, &loss(&1, x, y))

    {
      w - grad_w * @learning_rate,
      b - grad_b * @learning_rate
    }
  end
```

---

# Update

![png](images/forward.png)

---

<video width="700" height="700" controls autoplay>
  <source src="images/manual.mov" type="video/mp4">
</video>

---

# Training - All Together Now

- a convenience function to loop over the data and update the parameters (w and b)

```elixir
  def train(data, epochs \\ 100) do
    Enum.reduce(1..epochs, random_parameters(), fn _i, acc ->
      data
      |> Enum.take(@batch_size)
      |> Enum.reduce(acc, fn batch, activations ->
          {x, y} = Enum.unzip(batch)
          x = Nx.tensor(x)
          y = Nx.tensor(y)
          update(activations, x, y)
        end)
    end)
  end
 ``` 

---

![png](images/fin.png)

---

# Axon

### A high-level interface for creating neural network models

---

# A Neural Network

![png](images/nn.png)

---

# Activations

## tanh

![png](images/tanh.png)

- non-linear functions that are applied to the output of the weights + bias

---



# Axon

```elixir
x1_input = Axon.input("x1", shape: {nil, 1})

model =
  x1_input
  |> Axon.dense(4, activation: :relu)
  |> Axon.dense(1, activation: :tanh)

batch_size = 64

data =
  Stream.repeatedly(fn ->
    x = Nx.random_uniform({batch_size, 1}, 0, 50)

    {%{"x1" => x}, Nx.tanh(x)}
  end)

params =
  model
  |> Axon.Loop.trainer(:mean_squared_error, :sgd)
  |> Axon.Loop.run(data, %{}, epochs: 100, iterations: 1000)
```

---

# Conclusion

---

# Additional Resources

- [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) -- Andrej Karpathy
- [fast.ai](https://www.fast.ai/) -- Jeremy Howard
- [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work) -- Jeremy Howard
- [Up and Running Nx](https://dockyard.com/blog/2021/04/08/up-and-running-nx) -- Dockyard, Sean Moriarity
- [My LiveBook GH Repo](https://github.com/iacutone/presentations/tree/master/sgd)

---

class: center, middle

# FIN


    </textarea>
    <script src="../remark.min.js"></script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>


<style>
  img {
    max-width: 85%;
    max-height: 85%;
 }

  .lg img {
    position: absolute;
    object-fit: cover;
    width: 100%;
  }
</style>
