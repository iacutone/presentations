<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Learn Stochastic Gradient Descent in 30 Minutes

![jpeg](images/qr_code.png)

---

# Agenda

1. Welcome
1. Intro
1. Why?
1. First Principle
1. Recipes
1. Conclusion

---

# Welcome

![gif](images/hello.gif)

---

# Welcome

![jpeg](images/avatar.jpeg)

---
class: center, middle

# Intro


![svg](images/icon-cars-app.png)

---

# Learning From First Principles

![svg](images/before.jpeg)

---

# Learning From First Principles

![svg](images/after.jpeg)


# 10k Foot View

![png](images/nn.png)

---

# What does SGD mean?

--

## Stochastic:
- random

--

## Gradient:
- slope of a function

--

## Descent:
- moving downwards

---

## SGD in Action

![svg](images/sgd.gif)

---

# Why Learn Stochastic Gradient Descent?

--

- SGD is the building block of deep neural networks 

--

- SGD is arguably the most important piece of deep neural networks

--

- When ChatGPT predicts the next word in a sequence, a form of SGD was used the train the model.

---

# SGD Ingredients

## The Derivative

![svg](images/CodeCogsEqn.svg)

--

y = 5x

--

dy/dx = (5(x+h) - 5x) / h 

--

      = 5x + 5h - 5x / h

--

      = 5h / h

--

      = 5

--

In other words, the derivative tells us the sensitivity of the function to change.

---

# SGD Ingredients

## The Derivative

![svg](images/CodeCogsEqn.svg)

### Applied in SGD

- we will use this fundamental rule of calculus to calculate the gradient of a neural network.

--

- we will change _h_ a little to determine how a small changes effects the gradient, does the gradient increase or decrease?

---

# SGD Ingredients

## The Chain Rule

![png](images/chainrule.png)

--

"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man." - [George F. Simmons](https://en.wikipedia.org/wiki/Chain_rule)

--

We can take advatage of this rule to calculate the gradient of a neural network via the derivative.

---

# LiveBook Deep Dive

## Example Node Struct

```elixir
  @type t :: %Node{
    data: Integer.t() | nil,
    grad: Integer.t(),
    label: String.t(),
    _backward: fun(any),
    _op: String.t(),
    _children: list(Node{})
  }
```

---

# How is SGD using derivatives?

## Node Module API

### Addition

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.add(a, b)

# outputs -1 in data field
```

---

# How is SGD using derivatives?

## Node Module API

### Multiplication

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.mult(a, b)

# outputs -6 in data field
```

---

# How is SGD using derivatives?

### Multiplication then Addition, Graphed

![png](images/nodes.png)


```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
c = %Node1{data: 10, label: "c"}
d = Node1.mult(a, b, "e") |> Node1.add(c, "d", 1)
```

---

# How is SGD using derivatives?

### In terms of a

![png](images/nodes-back.png)
![png](images/a.png)

---

# How is SGD using derivatives?

### In terms of b

![png](images/nodes-back.png)
![png](images/b.png)

---

# How is SGD using derivatives?

### In terms of c

![png](images/nodes-back.png)
![png](images/c.png)

---

# Neural Network Ingredients

### Weights and Bias
### Activation
### Learning Rate
### Loss Function
### Backpropagation
### Forward Pass

---

# A Neuron

![png](images/neuron.png)

---

# A Neuron

### In code

![png](images/wv.png)

```elixir
x1 = %Node1{data: 2.0, label: "x1"}
x2 = %Node1{data: 0.0, label: "x2"}

w1 = %Node1{data: -3.0, label: "w1"}
w2 = %Node1{data: 1.0, label: "w2"}

b = %Node1{data: 6.881373587019541, label: "b"}

x1w1 = Node1.mult(x1, w1, "x1*w1")
x2w2 = Node1.mult(x2, w2, "x2*w2")
x1w1x2w2 = Node1.add(x1w1, x2w2, "x1*w1+x2*w2")
n = Node1.add(x1w1x2w2, b, "n")
o = Node1.tanh(n, "o", 1)
```

---

# A Neural Network

![png](images/nn.png)

---

# Nx: 

### Numerical Elixir

---

# Node module in Nx

```elixir
x1 = Nx.tensor([2.0])
x2 = Nx.tensor([0.0])

w1 = Nx.tensor([-3.0])
w2 = Nx.tensor([1.0])

b = Nx.tensor([6.8813735870195432])

x1w1 = Nx.multiply(x1,w1)
x2w2 = Nx.multiply(x2, w2)

output = x1w1 |> Nx.add(x2w2) |> Nx.add(b)
Nx.tanh(output)
```
---

# tanh function

![png](images/tanh.png)

--

```elixir
:math.tanh(-5) # -0.9999092042625951
:math.tanh(-1) # -0.7615941559557649
:math.tanh(0) # 0.0
:math.tanh(1) # -0.7615941559557649
:math.tanh(-5) # 0.9999092042625951
```

---

# Weights/Bias

- weights are random numbers that are multiplied by the input data


```elixir
  defn init_random_parameters do
    w = Nx.random_normal({}, 0.0, 1.0)
    b = Nx.random_normal({}, 0.0, 1.0)
    {w, b}
  end
```

--

```elixir
  activation = w * input + b
```

---

# Activations

- non-linear functions that are applied to the output of the weights + bias

--

- like tanh

---

# Learning Rate

- a hyperparameter that controls how much to update the weights

---

# Learning Rate

![png](images/lr-high.png)

---

# Learning Rate

![png](images/lr-low.png)

---

# Loss Function

- let's you know how far the prediction is off from the actual value

--

- mean squared error is a common loss function


--

```elixir
  defn loss(params, x, y) do
    y_hat = predict(params, x)

    (y - y_hat)
    |> Nx.pow(2)
    |> Nx.mean()
  end
```

---

# Backpropagation

- process of calculating the gradient of the neural network

```elixir
  defn backprop({w, b} = params, x, y) do
    {grad_w, grad_b} = grad(params, &loss(&1, x, y))

    {
      w - grad_w * @learning_rate,
      b - grad_b * @learning_rate
    }
  end
```

---

# Forward Pass

- loop accumulating the changes in gradients in the network


```elixir
  def train(data, epochs \\ 1000) do
    params = init_random_parameters()

    {x, y} = Enum.unzip(data)

    x = Nx.tensor(x)
    y = Nx.tensor(y)

    Enum.reduce(1..epochs, params, fn _i, acc ->
      backprop(acc, x, y)
    end)
  end
 ``` 

---

# Axon

### A high-level interface for creating neural network models

---

# Axon

```elixir
x1_input = Axon.input("x1", shape: {nil, 1})

model =
  x1_input
  |> Axon.dense(4, activation: :relu)
  |> Axon.dense(1, activation: :tanh)

batch_size = 64

data =
  Stream.repeatedly(fn ->
    x = Nx.random_uniform({batch_size, 1}, 0, 50)

    {%{"x1" => x}, Nx.tanh(x)}
  end)

params =
  model
  |> Axon.Loop.trainer(:mean_squared_error, :sgd)
  |> Axon.Loop.run(data, %{}, epochs: 100, iterations: 1000)
```

---

# Conclusion

---

# Additional Resources

- [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) -- Andrej Karpathy
- [fast.ai](https://www.fast.ai/) -- Jeremy Howard
- [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work) -- Jeremy Howard
- [My LiveBook GH Repo](https://github.com/iacutone/presentations/sgd)

---

class: center, middle

# FIN


    </textarea>
    <script src="../remark.min.js"></script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>


<style>
  img {
    max-width: 75%;
    max-height: 75%;
  }
</style>
