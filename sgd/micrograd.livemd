# micrograd

```elixir
Mix.install([
  {:axon, "~> 0.6.0"},
  :dg,
  # {:exla, "~> 0.4.0"},
  {:kino_vega_lite, "~> 0.1.9"}
])

# Nx.Defn.default_options(compiler: EXLA)
alias VegaLite, as: Vl
```

## Value Tree

```elixir
defmodule Node1 do
  @moduledoc "stores a single scalar value and its gradient"
  defstruct data: nil,
            grad: 0,
            label: "",
            _backward: nil,
            _op: "",
            _children: []

  @type t :: %Node1{
          data: Integer.t() | nil,
          grad: Integer.t(),
          label: String.t(),
          _backward: fun(any),
          _op: String.t(),
          _children: list(%Node1{})
        }

  def add(%Node1{} = left, %Node1{} = right, label \\ "add", grad \\ 0) do
    out = %Node1{
      data: left.data + right.data,
      grad: grad,
      _children: previous(left, right),
      _op: "+",
      label: label
    }

    backward = fn node ->
      left = %Node1{left | grad: left.grad + node.grad}
      right = %Node1{right | grad: right.grad + node.grad}

      %Node1{node | _children: previous(left, right)}
    end

    %Node1{out | _backward: backward}
  end

  def mult(%Node1{} = left, %Node1{} = right, label \\ "mult", grad \\ 0) do
    out = %Node1{
      data: left.data * right.data,
      grad: grad,
      _children: previous(left, right),
      _op: "*",
      label: label
    }

    backward = fn node ->
      left = %Node1{left | grad: right.data * node.grad}
      right = %Node1{right | grad: left.data * node.grad}

      %Node1{node | _children: previous(left, right)}
    end

    %Node1{out | _backward: backward}
  end

  def tanh(%Node1{data: data} = children, label \\ "tanh", grad \\ 1) do
    t = (:math.exp(2 * data) - 1) / (:math.exp(2 * data) + 1)

    out = %Node1{
      data: t,
      grad: grad,
      label: label,
      _children: [children],
      _op: "tanh"
    }

    backward = fn node ->
      children = %Node1{children | grad: 1 - t ** 2}
      %Node1{node | _children: [children]}
    end

    %Node1{out | _backward: backward}
  end

  def previous(left, right) do
    [left, right]
  end

  def backward(%Node1{_children: []} = root) do
    root
  end

  def backward(
        %Node1{
          data: data,
          grad: grad,
          label: label,
          _op: op,
          _backward: backward
        } = node
      ) do
    node =
      if node._backward do
        node._backward.(node)
      else
        node
      end

    updated_children =
      Enum.map(node._children, fn child ->
        backward(child)
      end)

    %Node1{
      data: data,
      grad: grad,
      label: label,
      _backward: backward,
      _op: op,
      _children: updated_children
    }
  end
end
```

## Interacting with the Value API

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
Node1.add(a, b)
```

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
c = %Node1{data: 10, label: "c"}
d = Node1.mult(a, b, "e") |> Node1.add(c, "d", 1)
```

## Add Value Tree Visualizations

```elixir
defmodule Graph do
  def draw_dot(root) do
    dot = DG.new()
    build_dot(root, dot, %{count: 0, ops: [], rids: []})

    dot
  end

  def build_dot(node, dot, visited) do
    label = "#{node.label} -> data #{node.data} -> grad #{node.grad}"
    rid = to_string(:rand.uniform(1000))

    DG.add_vertex(dot, node.label, label)

    count = Map.get(visited, :count)
    ops = Map.get(visited, :ops)
    rids = Map.get(visited, :rids)
    val = [node._op]
    visited = %{count: count + 1, ops: ops ++ val, rids: rids ++ [rid]}

    if node._op != "" do
      # create left _op vertex and connect to right edge
      DG.add_vertex(dot, "#{node._op}" <> rid, node._op)
      DG.add_edge(dot, "#{node._op}" <> rid, node.label)
    end

    if count != 0 do
      # if not root node, create edge between vertex and _op
      ops = Map.get(visited, :ops)
      op = Enum.at(ops, count - 1)
      rid = Enum.at(rids, count - 1)
      DG.add_edge(dot, String.trim(node.label), String.trim(op) <> rid)
    end

    Enum.map(node._children, fn child ->
      build_dot(child, dot, visited)
    end)
  end
end
```

```elixir
d
|> Node1.backward()
|> Graph.draw_dot()
```

```mermaid
graph LR
    b[b -> data -3 -> grad 2]-->*201[*]
    *201[*]-->e[e -> data -6 -> grad 1]
    e[e -> data -6 -> grad 1]-->+336[+]
    c[c -> data 10 -> grad 1]-->+336[+]
    a[a -> data 2 -> grad -3]-->*201[*]
    +336[+]-->d[d -> data 4 -> grad 1]
```

## What is h in the derivative?

```elixir
h = 0.0001

# inputs
a = %Node1{data: 2.0, label: "a"}
b = %Node1{data: -3.0, label: "b"}
c = %Node1{data: 10.0, label: "c"}

# what does it mean to nudge a?
d1 = Node1.mult(a, b) |> Node1.add(c)
a = %Node1{data: 2.0 + h}
d2 = Node1.mult(a, b) |> Node1.add(c)

IO.inspect(d1.data)
IO.inspect(d2.data)
IO.inspect((d2.data - d1.data) / h)
```

```elixir
h = 0.0001

# inputs
a = %Node1{data: 2.0, label: "a"}
b = %Node1{data: -3.0, label: "b"}
c = %Node1{data: 10.0, label: "c"}

# what does it mean to nudge b?
d1 = Node1.mult(a, b) |> Node1.add(c)
b = %Node1{data: -3.0 + h}
d2 = Node1.mult(a, b) |> Node1.add(c)

IO.inspect(d1.data)
IO.inspect(d2.data)
IO.inspect((d2.data - d1.data) / h)
```

```elixir
h = 0.0001

# inputs
a = %Node1{data: 2.0 + h, label: "a"}
b = %Node1{data: -3.0, label: "b"}
c = %Node1{data: 10.0, label: "c"}

# what does it mean to nudge c?
d1 = Node1.mult(a, b) |> Node1.add(c)
c = %Node1{data: 10.0 + h}
d2 = Node1.mult(a, b) |> Node1.add(c)

IO.inspect(d1.data)
IO.inspect(d2.data)
IO.inspect((d2.data - d1.data) / h)
```

## A More Complicated Value Tree

```elixir
a = %Node1{data: 2, label: "a"}
b = %Node1{data: -3, label: "b"}
c = %Node1{data: 10, label: "c"}
e = Node1.mult(a, b, "e")
d = Node1.add(e, c, "d")
f = %Node1{data: -2, label: "f"}
l = Node1.mult(d, f, "L", 1)

l
|> Node1.backward()
|> Graph.draw_dot()
```

```mermaid
graph LR
    f[f -> data -2 -> grad 4]-->*393[*]
    b[b -> data -3 -> grad -4]-->*699[*]
    *393[*]-->L[L -> data -8 -> grad 1]
    e[e -> data -6 -> grad -2]-->+920[+]
    *699[*]-->e[e -> data -6 -> grad -2]
    c[c -> data 10 -> grad -2]-->+920[+]
    a[a -> data 2 -> grad 6]-->*699[*]
    d[d -> data 4 -> grad -2]-->*393[*]
    +920[+]-->d[d -> data 4 -> grad -2]
```

## Value Tree with Weights

```elixir
x1 = %Node1{data: 2.0, label: "x1"}
x2 = %Node1{data: 0.0, label: "x2"}

w1 = %Node1{data: -3.0, label: "w1"}
w2 = %Node1{data: 1.0, label: "w2"}

b = %Node1{data: 6.881373587019541, label: "b"}

x1w1 = Node1.mult(x1, w1, "x1*w1")
x2w2 = Node1.mult(x2, w2, "x2*w2")
x1w1x2w2 = Node1.add(x1w1, x2w2, "x1*w1+x2*w2")
n = Node1.add(x1w1x2w2, b, "n")
o = Node1.tanh(n, "o", 1)

o
|> Node1.backward()
|> Graph.draw_dot()
```

```mermaid
graph LR
    +417[+]-->x1*w1+x2*w2[x1*w1+x2*w2 -> data -6.0 -> grad 0.5]
    b[b -> data 6.881373587019541 -> grad 0.50]-->+887[+]
    x1[x1 -> data 2.0 -> grad -1.5]-->*191[*]
    *558[*]-->x2*w2[x2*w2 -> data 0.0 -> grad 0.5]
    w2[w2 -> data 1.0 -> grad 0.0]-->*558[*]
    x1*w1[x1*w1 -> data -6.0 -> grad 0.5]-->+417[+]
    x1*w1+x2*w2[x1*w1+x2*w2 -> data -6.0 -> grad 0.5]-->+887[+]
    tanh583[tanh]-->o[o -> data 0.7071067811865467 -> grad 1]
    +887[+]-->n[n -> data 0.8813735870195414 -> grad 0.5]
    *191[*]-->x1*w1[x1*w1 -> data -6.0 -> grad 0.5]
    w1[w1 -> data -3.0 -> grad 1.0]-->*191[*]
    n[n -> data 0.8813735870195414 -> grad 0.5]-->tanh583[tanh]
    x2*w2[x2*w2 -> data 0.0 -> grad 0.5]-->+417[+]
    x2[x2 -> data 0.0 -> grad 0.5]-->*558[*]
```

## Intro to Nx

```elixir
x1 = Nx.tensor([2.0])
x2 = Nx.tensor([0.0])

w1 = Nx.tensor([-3.0])
w2 = Nx.tensor([1.0])

b = Nx.tensor([6.8813735870195432])

x1w1 = Nx.multiply(x1, w1)
x2w2 = Nx.multiply(x2, w2)

output = x1w1 |> Nx.add(x2w2) |> Nx.add(b)
Nx.tanh(output)
```

## Visualizing the tanh Function

```elixir
chart =
  Vl.new(width: 400, height: 400)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "x", type: :quantitative)
  |> Vl.encode_field(:y, "y", type: :quantitative)
  |> Kino.VegaLite.new()
  |> Kino.render()

for i <- -10..10 do
  point = %{x: i, y: :math.tanh(i)}
  Kino.VegaLite.push(chart, point)
  Process.sleep(25)
end
```

```elixir
IO.inspect(:math.tanh(-5))
IO.inspect(:math.tanh(-1))
IO.inspect(:math.tanh(0))
IO.inspect(:math.tanh(-1))
IO.inspect(:math.tanh(5))
```

## Solving for tanh using Nx

```elixir
defmodule Solver do
  import Nx.Defn

  @learning_rate 0.05

  defn predict({w, b}, x) do
    activation = w * x + b
    Nx.tanh(activation)
  end

  defn loss(params, x, y) do
    y_hat = predict(params, x)

    (y - y_hat)
    |> Nx.pow(2)
    |> Nx.mean()
  end

  defn backprop({w, b} = params, x, y) do
    {grad_w, grad_b} = grad(params, &loss(&1, x, y))

    {
      w - grad_w * @learning_rate,
      b - grad_b * @learning_rate
    }
  end

  defn init_random_parameters do
    w = Nx.random_normal({}, 0.0, 1.0)
    b = Nx.random_normal({}, 0.0, 1.0)

    {w, b}
  end

  def train(data, epochs \\ 1000) do
    params = init_random_parameters()

    {x, y} = Enum.unzip(data)

    x = Nx.tensor(x)
    y = Nx.tensor(y)

    Enum.reduce(1..epochs, params, fn _i, acc ->
      backprop(acc, x, y)
    end)
  end
end
```

```elixir
data =
  Enum.reduce(-20..20, [], fn i, acc ->
    i = i / 100
    acc ++ [{i, :math.tanh(i)}]
  end)

params = Solver.train(data)
```

```elixir
Solver.predict(params, -5.0)
```

## Solving for tanh using Axon

```elixir
# shape of the input
x1_input = Axon.input("x1", shape: {nil, 1})

model =
  x1_input
  |> Axon.dense(4, activation: :relu)
  |> Axon.dense(1, activation: :tanh)

batch_size = 64

data =
  Stream.repeatedly(fn ->
    x = Nx.random_uniform({batch_size, 1}, 0, 50)

    {%{"x1" => x}, Nx.tanh(x)}
  end)

params =
  model
  |> Axon.Loop.trainer(:mean_squared_error, :sgd)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(data, %{}, epochs: 100, iterations: 100)

Axon.predict(model, params, %{
  "x1" => Nx.tensor([[0.5]])
})
```

```elixir
:math.tanh(0.5)
```
