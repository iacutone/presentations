# micrograd

```elixir
Mix.install([
  :dg,
  {:kino, "~> 0.10.0"},
  {:kino_vega_lite, "~> 0.1.9"},
  {:vega_lite, "~> 0.1.6"}
])
```

## Section

```elixir
defmodule Value do
  @moduledoc "stores a single scalar value and its gradient"
  defstruct data: nil,
            grad: 0,
            label: "",
            _backward: nil,
            _op: "",
            _prev: []

  @type t :: %Value{
          data: Integer.t() | nil,
          grad: Integer.t(),
          label: String.t(),
          _backward: fun(any),
          _op: String.t(),
          _prev: [%Value{}]
        }

  def add(%Value{} = left, %Value{} = right, label: label) do
    out = %Value{
      data: left.data + right.data,
      _prev: previous(left, right),
      _op: "+",
      label: label
    }

    backward = fn node ->
      left = %Value{left | grad: left.grad + node.grad}
      right = %Value{right | grad: right.grad + node.grad}
      %Value{node | _prev: previous(left, right)}
    end

    %Value{out | _backward: backward}
  end

  def mult(%Value{} = left, %Value{} = right, label: label) do
    out = %Value{
      data: left.data * right.data,
      _prev: previous(left, right),
      _op: "*",
      label: label
    }

    backward = fn node ->
      left = %Value{left | grad: right.data * node.grad}
      right = %Value{right | grad: left.data * node.grad}
      %Value{node | _prev: previous(left, right)}
    end

    %Value{out | _backward: backward}
  end

  def tanh(%Value{data: data} = prev, label: label) do
    t = (:math.exp(2 * data) - 1) / (:math.exp(2 * data) + 1)

    out = %Value{
      data: t,
      label: label,
      _prev: [prev],
      _op: "tanh"
    }

    backward = fn node ->
      prev = %Value{prev | grad: 1 - t ** 2}
      %Value{node | _prev: [prev]}
    end

    %Value{out | _backward: backward}
  end

  def previous(left, right) do
    [left, right]
  end

  def backward(root) do
    gradients = recurse_backward(root, 0, %{})

    gradients
    |> Enum.reduce(root, fn {label, gradient}, acc ->
      update_node(acc, label, gradient)
    end)
  end

  def update_node(
        %Value{
          data: data,
          grad: _grad,
          label: label,
          _backward: backward,
          _op: op,
          _prev: prev
        },
        label,
        new_value
      ) do
    %Value{
      data: data,
      grad: new_value,
      label: label,
      _backward: backward,
      _op: op,
      _prev: prev
    }
  end

  def update_node(%Value{_prev: []} = root, _value, _new_value) do
    root
  end

  def update_node(
        %Value{
          data: data,
          grad: grad,
          label: label,
          _backward: backward,
          _op: op,
          _prev: prev
        },
        other_label,
        new_value
      ) do
    updated_children =
      Enum.map(prev, fn child ->
        update_node(child, other_label, new_value)
      end)

    %Value{
      data: data,
      grad: grad,
      label: label,
      _backward: backward,
      _op: op,
      _prev: updated_children
    }
  end

  def recurse_backward(node, count, counted) do
    {next, counted} =
      if count == 0 do
        # when last node, set grad to 1 for backprop
        {%Value{node | grad: 1}, Map.put(counted, node.label, 1)}
      else
        {node, counted}
      end

    if next && next._backward do
      next = next._backward.(next)
      # IO.inspect next, label: "next"

      Enum.reduce(next._prev, counted, fn
        %{_prev: [_left | _right]} = child, counted ->
          counted = Map.put(counted, child.label, child.grad)

          if child._op != "" do
            # if there is a child node to recurse on
            recurse_backward(child, count + 1, counted)
          end

        child, counted ->
          Map.put(counted, child.label, child.grad)
      end)
    end
  end
end
```

```elixir
a = %Value{data: 2, label: "a"}
b = %Value{data: -3, label: "b"}
c = %Value{data: 10, label: "c"}
d = Value.mult(a, b, label: "e") |> Value.add(c, label: "d")
```

```elixir
defmodule Graph do
  def draw_dot(root) do
    dot = DG.new()
    build_dot(root, dot, %{count: 0, ops: [], rids: []})

    dot
  end

  def build_dot(node, dot, visited) do
    label = "#{node.label} -> data #{node.data} -> grad #{node.grad}"
    rid = to_string(:rand.uniform(1000))

    DG.add_vertex(dot, node.label, label)

    count = Map.get(visited, :count)
    ops = Map.get(visited, :ops)
    rids = Map.get(visited, :rids)
    val = [node._op]
    visited = %{count: count + 1, ops: ops ++ val, rids: rids ++ [rid]}

    if node._op != "" do
      # create left _op vertex and connect to right edge
      DG.add_vertex(dot, "#{node._op}" <> rid, node._op)
      DG.add_edge(dot, "#{node._op}" <> rid, node.label)
    end

    if count != 0 do
      # if not root node, create edge between vertex and _op
      ops = Map.get(visited, :ops)
      op = Enum.at(ops, count - 1)
      rid = Enum.at(rids, count - 1)
      DG.add_edge(dot, node.label, op <> rid)
    end

    Enum.map(node._prev, fn child ->
      build_dot(child, dot, visited)
    end)
  end
end
```

```elixir
d
|> Value.backward()
|> Map.from_struct()
|> Graph.draw_dot()
```

```mermaid
graph LR
    b[b -> data -3 -> grad 2]-->*215[*]
    +164[+]-->d[d -> data 4 -> grad 1]
    *215[*]-->e[e -> data -6 -> grad 1]
    e[e -> data -6 -> grad 1]-->+164[+]
    c[c -> data 10 -> grad 1]-->+164[+]
    a[a -> data 2 -> grad -3]-->*215[*]
```

```elixir
a = %Value{data: 2, label: "a"}
b = %Value{data: -3, label: "b"}
c = %Value{data: 10, label: "c"}
e = Value.mult(a, b, label: "e")
d = Value.add(e, c, label: "d")
f = %Value{data: -2, label: "f"}
l = Value.mult(d, f, label: "L")

l
|> Value.backward()
|> Map.from_struct()
|> Graph.draw_dot()
```

```mermaid
graph LR
    f[f -> data -2 -> grad 4]-->*501[*]
    *650[*]-->e[e -> data -6 -> grad -2]
    b[b -> data -3 -> grad -4]-->*650[*]
    +918[+]-->d[d -> data 4 -> grad -2]
    *501[*]-->L[L -> data -8 -> grad 1]
    e[e -> data -6 -> grad -2]-->+918[+]
    c[c -> data 10 -> grad -2]-->+918[+]
    a[a -> data 2 -> grad 6]-->*650[*]
    d[d -> data 4 -> grad -2]-->*501[*]
```

```elixir
x1 = %Value{data: 2.0, label: "x1"}
x2 = %Value{data: 0.0, label: "x2"}

w1 = %Value{data: -3.0, label: "w1"}
w2 = %Value{data: 1.0, label: "w2"}

b = %Value{data: 6.881373587019541, label: "b"}

x1w1 = Value.mult(x1, w1, label: "x1*w1")
x2w2 = Value.mult(x2, w2, label: "x2*w2")
x1w1x2w2 = Value.add(x1w1, x2w2, label: "x1*w1+x2*w2")
n = Value.add(x1w1x2w2, b, label: "n")
o = Value.tanh(n, label: "o")

o
|> Value.backward()
|> Map.from_struct()
|> Graph.draw_dot()
```

```mermaid
graph LR
    +417[+]-->x1*w1+x2*w2[x1*w1+x2*w2 -> data -6.0 -> grad 0.5]
    b[b -> data 6.881373587019541 -> grad 0.50]-->+887[+]
    x1[x1 -> data 2.0 -> grad -1.5]-->*191[*]
    *558[*]-->x2*w2[x2*w2 -> data 0.0 -> grad 0.5]
    w2[w2 -> data 1.0 -> grad 0.0]-->*558[*]
    x1*w1[x1*w1 -> data -6.0 -> grad 0.5]-->+417[+]
    x1*w1+x2*w2[x1*w1+x2*w2 -> data -6.0 -> grad 0.5]-->+887[+]
    tanh583[tanh]-->o[o -> data 0.7071067811865467 -> grad 1]
    +887[+]-->n[n -> data 0.8813735870195414 -> grad 0.5]
    *191[*]-->x1*w1[x1*w1 -> data -6.0 -> grad 0.5]
    w1[w1 -> data -3.0 -> grad 1.0]-->*191[*]
    n[n -> data 0.8813735870195414 -> grad 0.5]-->tanh583[tanh]
    x2*w2[x2*w2 -> data 0.0 -> grad 0.5]-->+417[+]
    x2[x2 -> data 0.0 -> grad 0.5]-->*558[*]
```

```elixir
# defmodule Module do
#   def zero_grad(%Module{} = self) do
#     %Module{self | w: Enum.map(self.w, &(&1.zero_grad())), b: 0}
#   end

#   def parameters(_self) do
#     []
#   end
# end

defmodule Neuron do
  defstruct w: [], b: 0, nonlin?: true

  def new(nin, nonlin \\ true) do
    w = for _ <- 1..nin, do: Enum.random(-100..100) / 100
    %Neuron{w: w, b: 0, nonlin?: nonlin}
  end

  def call(%Neuron{w: w, b: b, nonlin?: nonlin}, x) do
    act = Enum.reduce(Enum.zip(w, x), b, fn {wi, xi}, acc -> acc + wi * xi end)
    if nonlin, do: relu(act), else: act
  end

  def parameters(%Neuron{w: w, b: b}) do
    w ++ [b]
  end

  defp relu(x) when x >= 0, do: x
  defp relu(_), do: 0
end

defmodule Layer do
  defstruct neurons: []

  def new(nin, nout, nonlin \\ true) do
    neurons = for _ <- 1..nout, do: Neuron.new(nin, nonlin)
    %Layer{neurons: neurons}
  end

  def call(%Layer{neurons: neurons}, x) do
    Enum.map(neurons, &Neuron.call(&1, x))
  end

  def parameters(%Layer{neurons: neurons}) do
    Enum.flat_map(neurons, &Neuron.parameters/1)
  end
end

defmodule MLP do
  defstruct layers: []

  def new(nin, nouts) do
    sz = [nin | nouts]

    layers =
      Enum.zip(sz, Enum.drop(sz, 1))
      |> Enum.map(fn {nin, nout} ->
        Layer.new(nin, nout, nonlin: nout != hd(nouts))
      end)

    %MLP{layers: layers}
  end

  def call(%MLP{layers: layers}, x) do
    Enum.reduce(layers, x, &Layer.call(&1, &2))
  end

  def parameters(%MLP{layers: layers}) do
    Enum.flat_map(layers, &Layer.parameters/1)
  end
end
```
